# âœ… VLM Political Steering Project - COMPLETE

## What Was Created

Successfully implemented a complete Jupyter notebook system for steering Qwen3-VL-32B responses along a political axis using image-based steering directions.

## Project Files

### Main Deliverable
ðŸ“ **[vlm_political/notebooks/steer_qwen_vl_political.ipynb](vlm_political/notebooks/steer_qwen_vl_political.ipynb)**
- 10-cell Jupyter notebook
- Complete end-to-end steering pipeline
- From image loading â†’ embedding extraction â†’ direction computation â†’ text generation
- Ready to run

### Documentation
ðŸ“ **[USAGE.md](USAGE.md)** - Complete setup & usage guide
ðŸ“ **[IMPLEMENTATION.md](IMPLEMENTATION.md)** - Technical specifications
ðŸ“ **[quick_reference.py](quick_reference.py)** - Reusable Python utilities
ðŸ“ **[README_COMPLETE.md](README_COMPLETE.md)** - Project overview

## Quick Start

### 1. Prepare Dataset
Place images in: `vlm_political/imgs/`
```
conservative/
  â”œâ”€â”€ abortion/ â†’ *.jpg, *.png
  â”œâ”€â”€ climate_change/ â†’ *.jpg, *.png
  â””â”€â”€ ...
liberal/
  â”œâ”€â”€ abortion/ â†’ *.jpg, *.png
  â”œâ”€â”€ climate_change/ â†’ *.jpg, *.png
  â””â”€â”€ ...
```

### 2. Run Notebook
```bash
cd /home/agi-admin/soo/ramneet/vlm_steering
jupyter notebook vlm_political/notebooks/steer_qwen_vl_political.ipynb
```

### 3. Execute All Cells
Run cells sequentially (1â†’10). Directions save automatically to `vlm_political/directions/`.

## Notebook Contents (10 Cells)

| # | Type | Purpose |
|----|------|---------|
| 1 | MD | Title, overview, dataset structure |
| 2 | PY | Imports + device setup (CUDA) |
| 3 | PY | Load Qwen3-VL-32B model |
| 4 | PY | Discover images from dataset |
| 5 | PY | Define image embedding extraction function |
| 6 | PY | Extract embeddings (conservative vs liberal) |
| 7 | PY | Compute steering direction (logistic regression) |
| 8 | PY | Define steering hooks & application |
| 9 | PY | Generate examples (baseline â†’ liberal â†’ conservative) |
| 10 | PY | Save directions to pickle + metadata to JSON |

## Key Features

âœ… **Image-Based Steering**
- Encodes political viewpoint in prompts: `"This image shows a [conservative|liberal] viewpoint"`
- Extracts embeddings from vision encoder
- Supports custom image datasets

âœ… **Direction Computation**
- **Logistic Regression**: Trains binary classifier (0=conservative, 1=liberal) on embeddings
- **Mean Difference**: Fallback method for direction computation
- Normalized, reusable vectors

âœ… **Hook-Based Steering**
- PyTorch `register_forward_hook()` mechanism
- Applied to decoder layers (middle & final)
- Coefficient control: negative (conservative), positive (liberal)

âœ… **Generation & Evaluation**
- Baseline generation (no steering)
- Liberal-steered generation (coefficient > 0)
- Conservative-steered generation (coefficient < 0)
- Example prompts: abortion, climate, healthcare, immigration

âœ… **Persistence**
- Saves directions as pickle files
- Metadata stored in JSON format
- Timestamped for tracking
- Ready for reuse across experiments

## Usage Examples

### Load & Apply Steering
```python
import pickle, torch
from pathlib import Path

# Load saved direction
with open('vlm_political/directions/conservative_liberal_direction_TIMESTAMP.pkl', 'rb') as f:
    data = pickle.load(f)
direction = torch.tensor(data['direction'])

# Apply to model
handles, _ = apply_steering_to_model(model, direction, coefficient=1.0)
outputs = model.generate(inputs, max_new_tokens=100)
for h in handles: h.remove()
```

### Generate with Steering
```python
# Baseline (no steering)
text_baseline = generate(prompt, coefficient=0.0)

# Liberal-steered
text_liberal = generate(prompt, coefficient=1.0)

# Conservative-steered
text_conservative = generate(prompt, coefficient=-1.0)
```

### Evaluate Coefficient Range
```python
for coef in [-2.0, -1.0, 0.0, 1.0, 2.0]:
    text = generate(prompt, coefficient=coef)
    print(f"Coef {coef}: {text[:100]}...")
```

## Technical Specifications

| Component | Details |
|-----------|---------|
| **Model** | Qwen/Qwen3-VL-32B (vision-language) |
| **Vision Encoder** | Processes images + text prompts |
| **Steering Layers** | Decoder layers (indices: mid, final) |
| **Direction Dim** | 3840 (hidden dimension) |
| **Embedding Method** | Mean pooling over sequence |
| **Hook Type** | PyTorch forward hook |
| **Direction Method** | Logistic regression (primary), mean-diff (fallback) |
| **Coefficient Range** | Typically Â±1.0 to Â±2.0 |

## Output Files (Generated on Run)

**Direction Vector**
```
vlm_political/directions/conservative_liberal_direction_20240115_143022.pkl
```
Contains: direction numpy array, shape, computation method, image counts

**Metadata**
```
vlm_political/directions/metadata_20240115_143022.json
```
Contains: model name, creation time, political axis, usage instructions

## Integration Points

- **Mirrors existing patterns** from `generation_utils.hook_model()`
- **Compatible with evaluation** scripts from `quantitative_comparisons/`
- **Same format** as existing steering vectors
- **Reproducible** with full metadata tracking

## Architecture Diagram

```
Image Dataset
    â†“
Image Embedding Extraction
    â†“ (conservative & liberal)
Embedding Collections
    â†“
Logistic Regression Classifier
    â†“
Steering Direction Vector
    â†“
Forward Hook Registration on Decoder Layers
    â†“
Text Generation with Steering
    â†“
Save Direction + Metadata
```

## What Makes This Special

1. **Vision Integration**: Explicitly encodes political viewpoint in image prompts
2. **Reproducible**: Complete metadata and timestamps
3. **Flexible**: Easy to extend for multiple topics/models
4. **Integrated**: Uses existing codebase patterns
5. **Well-Documented**: Comprehensive guides and examples
6. **Production-Ready**: Error handling, batch processing, proper resource management

## Next Steps

After running the notebook:

1. **Evaluate Steering Effectiveness**
   - Use political classifiers on generated text
   - Compare with baseline outputs
   - Measure steering strength

2. **Expand to Multiple Topics**
   - Compute separate directions for each topic
   - Test transfer across topics

3. **Optimize Hyperparameters**
   - Try different layer selections
   - Sweep coefficient values
   - Experiment with embedding aggregation methods

4. **Integrate with Evaluation**
   - Use `quantitative_comparisons/` scripts
   - Automated political bias measurement

5. **Advanced Analysis**
   - Vision encoder steering (in addition to decoder)
   - Direction interpolation/blending
   - Adversarial robustness testing

## File Locations

```
/home/agi-admin/soo/ramneet/vlm_steering/vlm_political/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ steer_qwen_vl_political.ipynb          â† Main notebook (10 cells)
â”œâ”€â”€ directions/
â”‚   â”œâ”€â”€ conservative_liberal_direction_*.pkl   â† Saved directions
â”‚   â””â”€â”€ metadata_*.json                        â† Computation metadata
â”œâ”€â”€ imgs/
â”‚   â”œâ”€â”€ conservative/                          â† Your images here
â”‚   â””â”€â”€ liberal/
â”œâ”€â”€ USAGE.md                                   â† Setup guide
â”œâ”€â”€ IMPLEMENTATION.md                          â† Technical spec
â”œâ”€â”€ quick_reference.py                         â† Utility functions
â””â”€â”€ README_COMPLETE.md                         â† This file
```

## Requirements

```
torch>=2.0
transformers>=4.30
pillow>=9.0
scikit-learn>=1.0
tqdm>=4.60
numpy>=1.20
```

Install with:
```bash
pip install torch transformers pillow scikit-learn tqdm numpy
```

## Status

âœ… **Notebook Creation**: Complete (10 cells)
âœ… **Documentation**: Complete (4 markdown files)
âœ… **Utilities**: Complete (quick_reference.py)
âœ… **Ready to Run**: Yes

## Support

1. **For setup issues**: See `USAGE.md`
2. **For technical details**: See `IMPLEMENTATION.md`
3. **For code examples**: See `quick_reference.py`
4. **For troubleshooting**: See `USAGE.md` (Troubleshooting section)

---

**Created**: 2024
**Model**: Qwen/Qwen3-VL-32B
**Task**: Political steering via image-based directions
**Status**: âœ… Complete and Ready
