{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a915fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "\n",
      "Loading Qwen/Qwen3-VL-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:12<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Loading and processing VLM dataset...\n",
      "Processing liberal: found 500 images, using 200\n",
      "Processing conservative: found 500 images, using 200\n",
      "\n",
      "Loaded 400 samples total.\n",
      "Liberal samples: 200\n",
      "Conservative samples: 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "project_root = notebook_path.parents[1]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model_id = \"Qwen/Qwen3-VL-32B-Instruct\"\n",
    "\n",
    "print(f\"\\nLoading {model_id}...\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True,\n",
    "    # Remove flash_attention_2 - it will use default attention instead\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "model_name = \"qwen3_vl_32b\"\n",
    "print(\"Model loaded successfully!\\n\")\n",
    "\n",
    "# Rest of your code...\n",
    "dataset_path = Path(\"../imgs\")\n",
    "\n",
    "def prepare_vlm_dataset(dataset_path, num_samples=200):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    categories = {\n",
    "        'liberal': {'label': 0, 'desc': 'left and liberal viewpoint'},\n",
    "        'conservative': {'label': 1, 'desc': 'right and conservative viewpoint'}\n",
    "    }\n",
    "    \n",
    "    for cat_name, info in categories.items():\n",
    "        cat_dir = dataset_path / cat_name\n",
    "        if not cat_dir.exists():\n",
    "            print(f\"Warning: {cat_dir} does not exist\")\n",
    "            continue\n",
    "        \n",
    "        img_files = []\n",
    "        for ext in ['jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG']:\n",
    "            img_files.extend(cat_dir.rglob(f\"*.{ext}\"))\n",
    "        \n",
    "        np.random.shuffle(img_files)\n",
    "        print(f\"Processing {cat_name}: found {len(img_files)} images, using {min(num_samples, len(img_files))}\")\n",
    "        \n",
    "        for img_path in img_files[:num_samples]:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                prompt_text = f\"This image shows a {info['desc']}.\"\n",
    "                \n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"image\"}, \n",
    "                        {\"type\": \"text\", \"text\": prompt_text}\n",
    "                    ]}\n",
    "                ]\n",
    "                \n",
    "                text = processor.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "                \n",
    "                processed = processor(\n",
    "                    text=[text], \n",
    "                    images=[img], \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                inputs.append(processed)\n",
    "                labels.append(info['label'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                \n",
    "    return inputs, labels\n",
    "\n",
    "print(\"Loading and processing VLM dataset...\")\n",
    "vlm_inputs, vlm_labels = prepare_vlm_dataset(dataset_path)\n",
    "print(f\"\\nLoaded {len(vlm_inputs)} samples total.\")\n",
    "print(f\"Liberal samples: {vlm_labels.count(0)}\")\n",
    "print(f\"Conservative samples: {vlm_labels.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42177224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components: 5\n",
      "Hidden layers KA: [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31, -32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47, -48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63]\n",
      "Hidden layers: [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31, -32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47, -48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63]\n",
      "\n",
      "Controller hyperparameters:\n",
      "control_method       : rfm\n",
      "rfm_iters            : 8\n",
      "forward_batch_size   : 1\n",
      "M_batch_size         : 2048\n",
      "n_components         : 5\n",
      "calibrate            : False\n",
      "\n",
      "Success! Controller initialized with 63 layers.\n",
      "Layer range: -1 to -63\n"
     ]
    }
   ],
   "source": [
    "from neural_controllers import NeuralController\n",
    "\n",
    "class VLMNeuralController(NeuralController):\n",
    "    def __init__(self, model, tokenizer, **kwargs):\n",
    "        # 1. Determine the correct layer count from the nested config\n",
    "        if hasattr(model.config, \"text_config\"):\n",
    "            self.num_layers = model.config.text_config.num_hidden_layers\n",
    "        else:\n",
    "            # Fallback for standard LLMs or other Qwen versions\n",
    "            self.num_layers = getattr(model.config, \"num_hidden_layers\", \n",
    "                              getattr(model.config, \"num_layers\", 40))\n",
    "\n",
    "        # 2. Temporarily patch model.config so the parent __init__ doesn't crash\n",
    "        # We add the attribute the parent class is looking for\n",
    "        if not hasattr(model.config, \"num_hidden_layers\"):\n",
    "            setattr(model.config, \"num_hidden_layers\", self.num_layers)\n",
    "\n",
    "        # 3. Call parent init (now it will find model.config.num_hidden_layers)\n",
    "        super().__init__(model, tokenizer, **kwargs)\n",
    "        \n",
    "        # 4. Ensure our hidden_layers list is exactly what we want\n",
    "        self.hidden_layers = list(range(-1, -self.num_layers, -1))\n",
    "\n",
    "    def compute_vlm_directions(self, vlm_inputs, labels):\n",
    "        \"\"\"\n",
    "        Custom computation loop using the is_vlm flag for direction_utils.\n",
    "        \"\"\"\n",
    "        self.directions, self.signs, self.detector_coefs, _ = self.toolkit._compute_directions(\n",
    "            vlm_inputs, \n",
    "            labels, \n",
    "            None, None, # val data\n",
    "            self.model, \n",
    "            self.tokenizer, \n",
    "            self.hidden_layers, \n",
    "            self.hyperparams,\n",
    "            is_vlm=True \n",
    "        )\n",
    "\n",
    "# Initialize\n",
    "vlm_controller = VLMNeuralController(\n",
    "    model, \n",
    "    processor.tokenizer, \n",
    "    control_method='rfm', \n",
    "    rfm_iters=8, \n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "print(f\"Success! Controller initialized with {len(vlm_controller.hidden_layers)} layers.\")\n",
    "print(f\"Layer range: {vlm_controller.hidden_layers[0]} to {vlm_controller.hidden_layers[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e151dba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Collecting image paths...\n",
      "============================================================\n",
      "Found 27 images in liberal/Welfare\n",
      "Found 21 images in liberal/isis\n",
      "Found 30 images in liberal/unemployment\n",
      "Found 18 images in liberal/black lives matter\n",
      "Found 27 images in liberal/racism\n",
      "Found 29 images in liberal/Border Security\n",
      "Found 27 images in liberal/terrorism\n",
      "Found 25 images in liberal/abortion\n",
      "Found 18 images in liberal/Minimum Wage\n",
      "Found 23 images in liberal/war on drugs\n",
      "Found 4 images in liberal/Vaccines\n",
      "Found 24 images in liberal/Fracking\n",
      "Found 23 images in liberal/lgbt\n",
      "Found 39 images in liberal/religion\n",
      "Found 11 images in liberal/Homelessness\n",
      "Found 32 images in liberal/immigration\n",
      "Found 32 images in liberal/Climate Change\n",
      "Found 26 images in liberal/Animal Rights\n",
      "Found 35 images in liberal/blue lives matter\n",
      "Found 29 images in liberal/Gun Control\n",
      "Found 27 images in conservative/Welfare\n",
      "Found 26 images in conservative/isis\n",
      "Found 40 images in conservative/unemployment\n",
      "Found 24 images in conservative/black lives matter\n",
      "Found 29 images in conservative/racism\n",
      "Found 27 images in conservative/Border Security\n",
      "Found 33 images in conservative/terrorism\n",
      "Found 47 images in conservative/abortion\n",
      "Found 16 images in conservative/Minimum Wage\n",
      "Found 20 images in conservative/war on drugs\n",
      "Found 4 images in conservative/Vaccines\n",
      "Found 4 images in conservative/Fracking\n",
      "Found 20 images in conservative/lgbt\n",
      "Found 44 images in conservative/religion\n",
      "Found 5 images in conservative/Homelessness\n",
      "Found 27 images in conservative/immigration\n",
      "Found 34 images in conservative/Climate Change\n",
      "Found 16 images in conservative/Animal Rights\n",
      "Found 32 images in conservative/blue lives matter\n",
      "Found 25 images in conservative/Gun Control\n",
      "\n",
      "Total liberal images found: 500\n",
      "Total conservative images found: 500\n",
      "\n",
      "============================================================\n",
      "STEP 2: Processing images for activation extraction...\n",
      "============================================================\n",
      "\n",
      "Processing liberal images: 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "liberal:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "liberal: 100%|██████████| 200/200 [00:02<00:00, 74.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 200 liberal images\n",
      "\n",
      "Processing conservative images: 200 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conservative: 100%|██████████| 200/200 [00:01<00:00, 137.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 200 conservative images\n",
      "\n",
      "============================================================\n",
      "STEP 2 COMPLETE: Dataset ready!\n",
      "============================================================\n",
      "Total samples: 400\n",
      "Liberal samples: 200\n",
      "Conservative samples: 200\n",
      "\n",
      "============================================================\n",
      "STEP 3: Computing steering directions...\n",
      "============================================================\n",
      "Computing directions for 400 samples...\n",
      "Label distribution: 200 vs 200\n",
      "Tuning metric: auc\n",
      "Error during direction computation: Could not infer dtype of NoneType\n",
      "\n",
      "Trying alternative format...\n",
      "Tuning metric: auc\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 60\u001b[0m, in \u001b[0;36mVLMNeuralController.compute_vlm_directions\u001b[0;34m(self, vlm_inputs, labels)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirections, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector_coefs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoolkit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_directions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass formatted data instead of raw inputs\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Labels are now embedded in formatted_data\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# val data\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:137\u001b[0m, in \u001b[0;36mRFMToolkit._compute_directions\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, model, tokenizer, hidden_layers, hyperparams, test_data, test_labels, device, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Process data and extract hidden states\u001b[39;00m\n\u001b[1;32m    136\u001b[0m (train_hidden_states, val_hidden_states, test_hidden_states, \n\u001b[0;32m--> 137\u001b[0m  train_y, val_y, test_y, test_data_provided, num_classes) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m direction_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m    147\u001b[0m }\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:35\u001b[0m, in \u001b[0;36mToolkit.preprocess_data\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, test_data, test_labels, model, tokenizer, hidden_layers, hyperparams, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_labels, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 35\u001b[0m     train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val_labels, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 157\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTEP 3: Computing steering directions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mvlm_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_vlm_directions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvlm_train_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvlm_train_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Directions computed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[18], line 74\u001b[0m, in \u001b[0;36mVLMNeuralController.compute_vlm_directions\u001b[0;34m(self, vlm_inputs, labels)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrying alternative format...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Alternative: pass as separate arguments\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirections, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector_coefs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoolkit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_directions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvlm_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# val data\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:137\u001b[0m, in \u001b[0;36mRFMToolkit._compute_directions\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, model, tokenizer, hidden_layers, hyperparams, test_data, test_labels, device, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning metric:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tuning_metric)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Process data and extract hidden states\u001b[39;00m\n\u001b[1;32m    136\u001b[0m (train_hidden_states, val_hidden_states, test_hidden_states, \n\u001b[0;32m--> 137\u001b[0m  train_y, val_y, test_y, test_data_provided, num_classes) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m direction_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m    147\u001b[0m }\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_data_provided:\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:57\u001b[0m, in \u001b[0;36mToolkit.preprocess_data\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, test_data, test_labels, model, tokenizer, hidden_layers, hyperparams, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m         train_data \u001b[38;5;241m=\u001b[39m [train_data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices]\n\u001b[1;32m     56\u001b[0m     all_y \u001b[38;5;241m=\u001b[39m train_labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 57\u001b[0m     train_y \u001b[38;5;241m=\u001b[39m \u001b[43mall_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m     val_y \u001b[38;5;241m=\u001b[39m all_y[val_indices]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Define the collection function\n",
    "def collect_image_paths(dataset_path, num_samples=200):\n",
    "    \"\"\"\n",
    "    Collects image paths from the dataset directory.\n",
    "    Handles deeply nested directory structures.\n",
    "    \"\"\"\n",
    "    image_data = {\n",
    "        'liberal': {},\n",
    "        'conservative': {}\n",
    "    }\n",
    "    \n",
    "    for viewpoint in ['liberal', 'conservative']:\n",
    "        viewpoint_dir = dataset_path / viewpoint\n",
    "        if not viewpoint_dir.exists():\n",
    "            print(f\"Warning: {viewpoint_dir} does not exist\")\n",
    "            continue\n",
    "        \n",
    "        # Get all image files recursively, regardless of depth\n",
    "        all_images = []\n",
    "        \n",
    "        # Use rglob with case-insensitive matching\n",
    "        for pattern in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "            all_images.extend(viewpoint_dir.rglob(pattern))\n",
    "        \n",
    "        if all_images:\n",
    "            # Group by topic folder (first level subdirectory)\n",
    "            for img_path in all_images:\n",
    "                # Get the topic name (first subdirectory under liberal/conservative)\n",
    "                try:\n",
    "                    relative = img_path.relative_to(viewpoint_dir)\n",
    "                    topic_name = relative.parts[0]\n",
    "                    \n",
    "                    if topic_name not in image_data[viewpoint]:\n",
    "                        image_data[viewpoint][topic_name] = []\n",
    "                    \n",
    "                    image_data[viewpoint][topic_name].append(img_path)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Print summary\n",
    "            for topic, paths in image_data[viewpoint].items():\n",
    "                print(f\"Found {len(paths)} images in {viewpoint}/{topic}\")\n",
    "        else:\n",
    "            print(f\"Warning: No images found in {viewpoint_dir}\")\n",
    "    \n",
    "    return image_data\n",
    "\n",
    "# Step 2: Define the processing function\n",
    "def get_vlm_activations(image_data, processor, num_samples_per_class=100):\n",
    "    \"\"\"\n",
    "    Processes the raw image paths into tensors ready for activation extraction.\n",
    "    Samples evenly across topics and includes topic in prompt.\n",
    "    \"\"\"\n",
    "    processed_inputs = []\n",
    "    labels = []\n",
    "\n",
    "    # Map your categories to binary labels\n",
    "    mapping = {'liberal': 0, 'conservative': 1}\n",
    "\n",
    "    for viewpoint, topics in image_data.items():\n",
    "        if viewpoint not in mapping:\n",
    "            continue\n",
    "        \n",
    "        # Flatten all images with their topics\n",
    "        all_imgs_with_topics = []\n",
    "        for topic_name, img_paths in topics.items():\n",
    "            for img_path in img_paths:\n",
    "                all_imgs_with_topics.append((img_path, topic_name))\n",
    "        \n",
    "        # Shuffle and sample\n",
    "        np.random.shuffle(all_imgs_with_topics)\n",
    "        sampled = all_imgs_with_topics[:num_samples_per_class]\n",
    "        \n",
    "        print(f\"\\nProcessing {viewpoint} images: {len(sampled)} samples\")\n",
    "        \n",
    "        for img_path, topic_name in tqdm(sampled, desc=f\"{viewpoint}\"):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                # Create topic-specific prompt\n",
    "                clean_topic = topic_name.lower()\n",
    "                prompt_text = f\"This photo represents a {viewpoint} standpoint on {clean_topic}\"\n",
    "                \n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"image\"}, \n",
    "                        {\"type\": \"text\", \"text\": prompt_text}\n",
    "                    ]}\n",
    "                ]\n",
    "                \n",
    "                text = processor.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=False\n",
    "                )\n",
    "                \n",
    "                inputs = processor(text=[text], images=[img], return_tensors=\"pt\")\n",
    "                \n",
    "                processed_inputs.append(inputs)\n",
    "                labels.append(mapping[viewpoint])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Successfully processed {sum(1 for l in labels if l == mapping[viewpoint])} {viewpoint} images\")\n",
    "\n",
    "    return processed_inputs, labels\n",
    "\n",
    "# Step 3: Actually collect and process the data\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: Collecting image paths...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset_path = Path(\"../imgs\")\n",
    "image_data = collect_image_paths(dataset_path)\n",
    "\n",
    "# Print summary\n",
    "total_liberal = sum(len(paths) for paths in image_data['liberal'].values())\n",
    "total_conservative = sum(len(paths) for paths in image_data['conservative'].values())\n",
    "print(f\"\\nTotal liberal images found: {total_liberal}\")\n",
    "print(f\"Total conservative images found: {total_conservative}\")\n",
    "\n",
    "if total_liberal == 0 and total_conservative == 0:\n",
    "    print(\"\\n❌ ERROR: No images found!\")\n",
    "    print(\"Checking if path is correct...\")\n",
    "    print(f\"Dataset path: {dataset_path.resolve()}\")\n",
    "    print(f\"Path exists: {dataset_path.exists()}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Processing images for activation extraction...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    vlm_train_inputs, vlm_train_labels = get_vlm_activations(\n",
    "        image_data, \n",
    "        processor, \n",
    "        num_samples_per_class=200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STEP 2 COMPLETE: Dataset ready!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples: {len(vlm_train_inputs)}\")\n",
    "    print(f\"Liberal samples: {vlm_train_labels.count(0)}\")\n",
    "    print(f\"Conservative samples: {vlm_train_labels.count(1)}\")\n",
    "    \n",
    "    if len(vlm_train_inputs) > 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"STEP 3: Computing steering directions...\")\n",
    "        print(\"=\"*60)\n",
    "        vlm_controller.compute_vlm_directions(vlm_train_inputs, vlm_train_labels)\n",
    "        print(\"\\n✅ Directions computed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n❌ ERROR: No samples were processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5df30dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing steering directions...\n",
      "Computing directions for 400 samples...\n",
      "Label distribution: 200 vs 200\n",
      "Tuning metric: auc\n",
      "Error during direction computation: Could not infer dtype of NoneType\n",
      "\n",
      "Trying alternative format...\n",
      "Tuning metric: auc\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 60\u001b[0m, in \u001b[0;36mVLMNeuralController.compute_vlm_directions\u001b[0;34m(self, vlm_inputs, labels)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirections, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector_coefs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoolkit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_directions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass formatted data instead of raw inputs\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Labels are now embedded in formatted_data\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# val data\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:137\u001b[0m, in \u001b[0;36mRFMToolkit._compute_directions\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, model, tokenizer, hidden_layers, hyperparams, test_data, test_labels, device, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Process data and extract hidden states\u001b[39;00m\n\u001b[1;32m    136\u001b[0m (train_hidden_states, val_hidden_states, test_hidden_states, \n\u001b[0;32m--> 137\u001b[0m  train_y, val_y, test_y, test_data_provided, num_classes) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m direction_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m    147\u001b[0m }\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:35\u001b[0m, in \u001b[0;36mToolkit.preprocess_data\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, test_data, test_labels, model, tokenizer, hidden_layers, hyperparams, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_labels, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 35\u001b[0m     train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val_labels, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now try computing directions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mComputing steering directions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvlm_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_vlm_directions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvlm_train_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvlm_train_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirections computed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 74\u001b[0m, in \u001b[0;36mVLMNeuralController.compute_vlm_directions\u001b[0;34m(self, vlm_inputs, labels)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrying alternative format...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Alternative: pass as separate arguments\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirections, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector_coefs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoolkit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_directions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvlm_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# val data\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_vlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:137\u001b[0m, in \u001b[0;36mRFMToolkit._compute_directions\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, model, tokenizer, hidden_layers, hyperparams, test_data, test_labels, device, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning metric:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tuning_metric)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Process data and extract hidden states\u001b[39;00m\n\u001b[1;32m    136\u001b[0m (train_hidden_states, val_hidden_states, test_hidden_states, \n\u001b[0;32m--> 137\u001b[0m  train_y, val_y, test_y, test_data_provided, num_classes) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m direction_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[1;32m    147\u001b[0m }\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_data_provided:\n",
      "File \u001b[0;32m~/soo/ramneet/vlm_steering/control_toolkits.py:57\u001b[0m, in \u001b[0;36mToolkit.preprocess_data\u001b[0;34m(self, train_data, train_labels, val_data, val_labels, test_data, test_labels, model, tokenizer, hidden_layers, hyperparams, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m         train_data \u001b[38;5;241m=\u001b[39m [train_data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices]\n\u001b[1;32m     56\u001b[0m     all_y \u001b[38;5;241m=\u001b[39m train_labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 57\u001b[0m     train_y \u001b[38;5;241m=\u001b[39m \u001b[43mall_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m     val_y \u001b[38;5;241m=\u001b[39m all_y[val_indices]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Now try computing directions\n",
    "print(\"\\nComputing steering directions...\")\n",
    "vlm_controller.compute_vlm_directions(vlm_train_inputs, vlm_train_labels)\n",
    "print(\"Directions computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a99cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute directions across all 63 layers\n",
    "vlm_controller.compute_vlm_directions(vlm_train_inputs, vlm_train_labels)\n",
    "\n",
    "# Save the political steering directions\n",
    "vlm_controller.save(\n",
    "    concept='political_lean', \n",
    "    model_name='qwen3_32b_vlm', \n",
    "    path='../directions/'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi_nc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
