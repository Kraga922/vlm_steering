{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Political Steering: Qwen3-VL-32B\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates steering Qwen3-VL-32B responses along a political axis using image-based steering directions.\n",
    "\n",
    "### Dataset Structure\n",
    "```\n",
    "vlm_political/imgs/\n",
    "├── conservative/\n",
    "│   ├── website_name_1/ → *.jpg, *.png\n",
    "│   ├── website_name_2/ → *.jpg, *.png\n",
    "│   └── ...\n",
    "└── liberal/\n",
    "    ├── website_name_1/ → *.jpg, *.png\n",
    "    ├── website_name_2/ → *.jpg, *.png\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "### Key Concept\n",
    "\n",
    "We compute a steering vector by:\n",
    "1. Extracting embeddings from conservative vs liberal political images (organized by source/website)\n",
    "2. Using logistic regression to find the political axis\n",
    "3. Applying the direction to model hidden states via forward hooks\n",
    "4. Generating text with controlled political bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "notebook_path = Path().resolve()\n",
    "project_root = notebook_path.parents[1]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from neural_controllers import NeuralController\n",
    "\n",
    "# Set GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen3-VL-32B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-VL-32B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForImageTextToText\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen3_vl_32b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/modeling_utils.py:4971\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4968\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[1;32m   4969\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[1;32m   4970\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4971\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4973\u001b[0m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[1;32m   4974\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1279\u001b[0m, in \u001b[0;36mQwen3VLForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m Qwen3VLModel(config)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2076\u001b[0m, in \u001b[0;36mPreTrainedModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;66;03m# Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\u001b[39;00m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;66;03m# setting it recursively)\u001b[39;00m\n\u001b[0;32m-> 2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation_internal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_adjust_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_init_check\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;66;03m# for initialization of the loss\u001b[39;00m\n\u001b[1;32m   2081\u001b[0m loss_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2686\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_adjust_attn_implementation\u001b[0;34m(self, attn_implementation, is_init_check)\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2685\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2686\u001b[0m     applicable_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_correct_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapplicable_attn_implementation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_init_check\u001b[49m\n\u001b[1;32m   2688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2689\u001b[0m     \u001b[38;5;66;03m# preload flash attention here to allow compile with fullgraph\u001b[39;00m\n\u001b[1;32m   2690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m applicable_attn_implementation\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2714\u001b[0m, in \u001b[0;36mPreTrainedModel.get_correct_attn_implementation\u001b[0;34m(self, requested_attention, is_init_check)\u001b[0m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;66;03m# Perform relevant checks\u001b[39;00m\n\u001b[1;32m   2713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m applicable_attention \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2714\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flash_attn_2_can_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_init_check\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m applicable_attention \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2716\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flash_attn_3_can_dispatch(is_init_check)\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2432\u001b[0m, in \u001b[0;36mPreTrainedModel._flash_attn_2_can_dispatch\u001b[0;34m(self, is_init_check)\u001b[0m\n\u001b[1;32m   2428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m you need flash_attn package version to be greater or equal than 2.1.0. Detected version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflash_attention_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2430\u001b[0m     )\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m-> 2432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2434\u001b[0m     )\n\u001b[1;32m   2435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Flash Attention 2 is not available. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Set visibility BEFORE importing torch/transformers\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "\n",
    "model_name = \"Qwen/Qwen3-VL-32B-Instruct\"\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\", # Automatically balances layers across GPUs 6 and 7\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" \n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 3. PREPARE INPUTS (Crucial Step)\n",
    "# This is where most people fail to use the GPU\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# In a real scenario, load your PIL image here\n",
    "# image = Image.open(...) \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Describe this image.\"}]}\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# Note: You'll need the actual image object here\n",
    "inputs = processor(text=[text], images=None, return_tensors=\"pt\")\n",
    "\n",
    "# 4. MOVE INPUTS TO DEVICE\n",
    "# 'model.device' will point to the first device in the map (cuda:0)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# 5. GENERATE\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "    print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: ../imgs\n",
      "Dataset exists: True\n",
      "\n",
      "=== Dataset Summary ===\n",
      "LIBERAL: 500 images across 20 sources\n",
      "  - Welfare: 27 images\n",
      "  - isis: 21 images\n",
      "  - unemployment: 30 images\n",
      "  - black lives matter: 18 images\n",
      "  - racism: 27 images\n",
      "CONSERVATIVE: 500 images across 20 sources\n",
      "  - Welfare: 27 images\n",
      "  - isis: 26 images\n",
      "  - unemployment: 40 images\n",
      "  - black lives matter: 24 images\n",
      "  - racism: 29 images\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing\n",
    "dataset_path = Path(\"../imgs\")  # relative to this notebook\n",
    "\n",
    "print(f\"Loading images from: {dataset_path}\")\n",
    "print(f\"Dataset exists: {dataset_path.exists()}\")\n",
    "\n",
    "# Organize images by political viewpoint\n",
    "# Structure: imgs/viewpoint/website_name/image.jpg\n",
    "image_data = defaultdict(lambda: defaultdict(list))  # {viewpoint: {website_source: [paths]}}\n",
    "\n",
    "if dataset_path.exists():\n",
    "    for subdir in dataset_path.iterdir():\n",
    "        if subdir.is_dir():\n",
    "            viewpoint = subdir.name  # 'conservative', 'liberal', etc.\n",
    "            # Iterate through website source directories\n",
    "            for website_dir in subdir.iterdir():\n",
    "                if website_dir.is_dir():\n",
    "                    website_source = website_dir.name  # website name\n",
    "                    # Collect all images in this website directory (recursively)\n",
    "                    for img_file in website_dir.rglob(\"*\"):\n",
    "                        if img_file.suffix.lower() in ['.jpg', '.png', '.jpeg']:\n",
    "                            image_data[viewpoint][website_source].append(img_file)\n",
    "\n",
    "    print(f\"\\n=== Dataset Summary ===\")\n",
    "    if image_data:\n",
    "        for viewpoint, sources in image_data.items():\n",
    "            total_imgs = sum(len(paths) for paths in sources.values())\n",
    "            print(f\"{viewpoint.upper()}: {total_imgs} images across {len(sources)} sources\")\n",
    "            for source, paths in list(sources.items())[:5]:\n",
    "                print(f\"  - {source}: {len(paths)} images\")\n",
    "    else:\n",
    "        print(\"⚠️ No images found under dataset_path. Check subdirectories and file extensions.\")\n",
    "else:\n",
    "    print(f\"⚠️ Dataset path not found: {dataset_path}\")\n",
    "    print(\"Please ensure vlm_political/imgs exists with conservative/ and liberal/ subdirectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Image embedding extraction function defined\n"
     ]
    }
   ],
   "source": [
    "def extract_image_embeddings(image_paths, batch_size=4, viewpoint_label=\"\"):\n",
    "    \"\"\"\n",
    "    Extract embeddings from images using the vision encoder.\n",
    "    This version moves tensors to device safely and retries tokenization\n",
    "    if the processor returns empty text tokens (common with some processors).\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    valid_paths = []\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=f\"Extracting {viewpoint_label} embeddings\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_imgs = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                batch_imgs.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to load {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not batch_imgs:\n",
    "            continue\n",
    "\n",
    "        # Include viewpoint context in prompts\n",
    "        prompts = [f\"This image shows a {viewpoint_label} viewpoint. Describe the content.\" for _ in batch_imgs]\n",
    "\n",
    "        try:\n",
    "            encoded = processor(\n",
    "                text=prompts,\n",
    "                images=batch_imgs,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "            )\n",
    "\n",
    "            # Move any tensor entries to device safely\n",
    "            for k, v in list(encoded.items()):\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    encoded[k] = v.to(device)\n",
    "\n",
    "            # Some processors may return empty text tokens when mixing images/text.\n",
    "            # If so, tokenize prompts separately and insert input_ids/attention_mask.\n",
    "            if (\"input_ids\" not in encoded) or (isinstance(encoded.get(\"input_ids\"), torch.Tensor) and encoded.get(\"input_ids\").numel() == 0):\n",
    "                try:\n",
    "                    tokenized = processor.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                    encoded[\"input_ids\"] = tokenized[\"input_ids\"].to(device)\n",
    "                    encoded[\"attention_mask\"] = tokenized[\"attention_mask\"].to(device)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Failed to tokenize prompts separately: {e}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    **encoded,\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "\n",
    "                if hasattr(outputs, \"hidden_states\") and outputs.hidden_states:\n",
    "                    hidden = outputs.hidden_states[-1]\n",
    "                    emb = hidden.mean(dim=1)\n",
    "                else:\n",
    "                    emb = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "                embeddings.append(emb.cpu())\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing batch: {e}\")\n",
    "            # Debug: print shapes/types of encoded items if available\n",
    "            try:\n",
    "                if 'encoded' in locals():\n",
    "                    for k, v in encoded.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            print(f\"  - {k}: {tuple(v.shape)}\")\n",
    "                        else:\n",
    "                            print(f\"  - {k}: {type(v)}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "    if embeddings:\n",
    "        embeddings = torch.cat(embeddings, dim=0)\n",
    "        return embeddings, valid_paths\n",
    "    else:\n",
    "        return torch.tensor([]), []\n",
    "\n",
    "print(\"✓ Image embedding extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Extracting Image Embeddings ===\n",
      "Conservative images: 500\n",
      "Liberal images: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:   2%|▏         | 3/125 [00:02<01:09,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 12785\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (51140, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1497\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (5988, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1079\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4316, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:   5%|▍         | 6/125 [00:02<00:31,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1481\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (5924, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1516\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6064, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1672\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6688, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:   6%|▋         | 8/125 [00:02<00:24,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1684\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6736, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1671\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6684, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:   8%|▊         | 10/125 [00:03<00:21,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 3309\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (13236, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1796\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (7184, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:   9%|▉         | 11/125 [00:03<00:20,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 517\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (2068, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1158\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4632, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  10%|█         | 13/125 [00:04<00:34,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 7431\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (29724, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  11%|█         | 14/125 [00:04<00:33,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 865\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3460, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1578\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6312, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  14%|█▍        | 18/125 [00:04<00:18,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2131\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (8524, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 871\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3484, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1532\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6128, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  16%|█▌        | 20/125 [00:04<00:14,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1042\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4168, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1283\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (5132, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  17%|█▋        | 21/125 [00:05<00:15,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2717\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (10868, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  19%|█▉        | 24/125 [00:05<00:17,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 3170\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (12680, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1584\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6336, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1457\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (5828, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  21%|██        | 26/125 [00:05<00:13,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1190\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4760, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 667\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (2668, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 638\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (2552, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  22%|██▏       | 28/125 [00:06<00:11,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 887\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3548, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 920\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3680, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  26%|██▌       | 32/125 [00:06<00:10,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2022\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (8088, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1320\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (5280, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1036\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4144, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  27%|██▋       | 34/125 [00:06<00:09,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1191\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4764, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1001\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4004, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  29%|██▉       | 36/125 [00:07<00:10,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2220\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (8880, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1874\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (7496, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  30%|███       | 38/125 [00:07<00:09,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1147\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4588, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 923\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3692, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  32%|███▏      | 40/125 [00:07<00:09,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2181\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (8724, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1222\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4888, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2160\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (8640, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  35%|███▌      | 44/125 [00:07<00:09,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1856\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (7424, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 956\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3824, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1596\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6384, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  36%|███▌      | 45/125 [00:08<00:09,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1696\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (6784, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  37%|███▋      | 46/125 [00:08<00:12,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1160\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4640, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  39%|███▉      | 49/125 [00:08<00:10,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2660\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (10640, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 854\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3416, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1225\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (4900, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  41%|████      | 51/125 [00:08<00:08,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 871\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (3484, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1464\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (5856, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting conservative embeddings:  42%|████▏     | 53/125 [00:09<00:12,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 2275\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (9100, 1536)\n",
      "  - image_grid_thw: (4, 3)\n",
      "⚠️ Error processing batch: Image features and image tokens do not match: tokens: 0, features 1437\n",
      "  - input_ids: (4, 11)\n",
      "  - attention_mask: (4, 11)\n",
      "  - pixel_values: (5748, 1536)\n",
      "  - image_grid_thw: (4, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiberal images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(liberal_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conservative_paths \u001b[38;5;129;01mand\u001b[39;00m liberal_paths:\n\u001b[0;32m---> 18\u001b[0m     cons_emb, cons_paths \u001b[38;5;241m=\u001b[39m \u001b[43mextract_image_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconservative_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mviewpoint_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconservative\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     lib_emb, lib_paths \u001b[38;5;241m=\u001b[39m extract_image_embeddings(\n\u001b[1;32m     24\u001b[0m         liberal_paths,\n\u001b[1;32m     25\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     26\u001b[0m         viewpoint_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliberal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConservative embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcons_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mextract_image_embeddings\u001b[0;34m(image_paths, batch_size, viewpoint_label)\u001b[0m\n\u001b[1;32m     27\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis image shows a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mviewpoint_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m viewpoint. Describe the content.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m batch_imgs]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_imgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Move any tensor entries to device safely\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(encoded\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/models/qwen3_vl/processing_qwen3_vl.py:163\u001b[0m, in \u001b[0;36mQwen3VLProcessor.__call__\u001b[0;34m(self, images, text, videos, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m output_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_kwargs(\n\u001b[1;32m    158\u001b[0m     Qwen3VLProcessorKwargs,\n\u001b[1;32m    159\u001b[0m     tokenizer_init_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39minit_kwargs,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     image_grid_thw \u001b[38;5;241m=\u001b[39m image_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_grid_thw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/image_processing_utils_fast.py:732\u001b[0m, in \u001b[0;36mBaseImageProcessorFast.__call__\u001b[0;34m(self, images, *args, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: ImageInput, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[DefaultFastImageProcessorKwargs]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:141\u001b[0m, in \u001b[0;36mQwen2VLImageProcessorFast.preprocess\u001b[0;34m(self, images, videos, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[Qwen2VLFastImageProcessorKwargs],\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/image_processing_utils_fast.py:757\u001b[0m, in \u001b[0;36mBaseImageProcessorFast.preprocess\u001b[0;34m(self, images, *args, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# Pop kwargs that are not needed in _preprocess\u001b[39;00m\n\u001b[1;32m    755\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_image_like_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:163\u001b[0m, in \u001b[0;36mQwen2VLImageProcessorFast._preprocess_image_like_inputs\u001b[0;34m(self, images, videos, do_convert_rgb, input_data_format, device, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_image_like_inputs(\n\u001b[1;32m    161\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages, do_convert_rgb\u001b[38;5;241m=\u001b[39mdo_convert_rgb, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    162\u001b[0m     )\n\u001b[0;32m--> 163\u001b[0m     batch_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Qwen2VLImageProcessorFast` works only with image inputs and doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt process videos anymore. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a deprecated behavior and will be removed in v5.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour videos should be forwarded to `Qwen2VLVideoProcessor`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:229\u001b[0m, in \u001b[0;36mQwen2VLImageProcessorFast._preprocess\u001b[0;34m(self, images, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, image_mean, image_std, patch_size, temporal_patch_size, merge_size, disable_grouping, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m resized_height, resized_width \u001b[38;5;241m=\u001b[39m stacked_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Fused rescale and normalize\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrescale_and_normalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstacked_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_std\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patches\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# add a temporal dimension if we have images\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     patches \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/image_processing_utils_fast.py:472\u001b[0m, in \u001b[0;36mBaseImageProcessorFast.rescale_and_normalize\u001b[0;34m(self, images, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# if/elif as we use fused rescale and normalize if both are set to True\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 472\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m do_rescale:\n\u001b[1;32m    474\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(images, rescale_factor)\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/transformers/image_processing_utils_fast.py:431\u001b[0m, in \u001b[0;36mBaseImageProcessorFast.normalize\u001b[0;34m(self, image, mean, std, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnormalize\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     image: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m        `torch.Tensor`: The normalized image.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/torchvision/transforms/v2/functional/_misc.py:32\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(inpt, mean, std, inplace)\u001b[0m\n\u001b[1;32m     29\u001b[0m _log_api_usage_once(normalize)\n\u001b[1;32m     31\u001b[0m kernel \u001b[38;5;241m=\u001b[39m _get_kernel(normalize, \u001b[38;5;28mtype\u001b[39m(inpt))\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/soo/abhinav/miniconda3/envs/phi_nc_env/lib/python3.10/site-packages/torchvision/transforms/v2/functional/_misc.py:67\u001b[0m, in \u001b[0;36mnormalize_image\u001b[0;34m(image, mean, std, inplace)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39msub(mean)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extract embeddings from conservative and liberal images\n",
    "print(\"\\n=== Extracting Image Embeddings ===\")\n",
    "\n",
    "conservative_paths = []\n",
    "liberal_paths = []\n",
    "\n",
    "for viewpoint, topics in image_data.items():\n",
    "    paths = [p for topic_paths in topics.values() for p in topic_paths]\n",
    "    if viewpoint.lower() == \"conservative\":\n",
    "        conservative_paths.extend(paths)\n",
    "    elif viewpoint.lower() == \"liberal\":\n",
    "        liberal_paths.extend(paths)\n",
    "\n",
    "print(f\"Conservative images: {len(conservative_paths)}\")\n",
    "print(f\"Liberal images: {len(liberal_paths)}\")\n",
    "\n",
    "if conservative_paths and liberal_paths:\n",
    "    cons_emb, cons_paths = extract_image_embeddings(\n",
    "        conservative_paths,\n",
    "        batch_size=4,\n",
    "        viewpoint_label=\"conservative\"\n",
    "    )\n",
    "    lib_emb, lib_paths = extract_image_embeddings(\n",
    "        liberal_paths,\n",
    "        batch_size=4,\n",
    "        viewpoint_label=\"liberal\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nConservative embeddings: {cons_emb.shape}\")\n",
    "    print(f\"Liberal embeddings: {lib_emb.shape}\")\n",
    "else:\n",
    "    print(\"⚠️ Insufficient images to compute directions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to import NeuralController: No module named 'neural_controllers'\n",
      "NeuralController unavailable — ensure neural_controllers.py is on PYTHONPATH.\n"
     ]
    }
   ],
   "source": [
    "# Use NeuralController to compute/apply directions from image embeddings\n",
    "# This cell attempts to reuse the project's `NeuralController` to compute directions\n",
    "# from `cons_emb` and `lib_emb` and then generate controlled outputs.\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from neural_controllers import NeuralController\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Failed to import NeuralController: {e}\")\n",
    "    NeuralController = None\n",
    "\n",
    "if NeuralController is None:\n",
    "    print(\"NeuralController unavailable — ensure neural_controllers.py is on PYTHONPATH.\")\n",
    "else:\n",
    "    # Ensure embeddings exist\n",
    "    if 'cons_emb' not in globals() or 'lib_emb' not in globals():\n",
    "        print(\"⚠️ cons_emb/lib_emb not found. Run the embedding extraction cells first.\")\n",
    "    else:\n",
    "        # Prepare training data\n",
    "        # Many of the toolkits expect 2D arrays [n_samples, dim]\n",
    "        cons_X = cons_emb.numpy() if isinstance(cons_emb, torch.Tensor) else np.array(cons_emb)\n",
    "        lib_X = lib_emb.numpy() if isinstance(lib_emb, torch.Tensor) else np.array(lib_emb)\n",
    "\n",
    "        X = np.vstack([cons_X, lib_X])\n",
    "        y = np.array([0]*len(cons_X) + [1]*len(lib_X))\n",
    "\n",
    "        print(f\"Prepared training data: X.shape={X.shape}, y.shape={y.shape}\")\n",
    "\n",
    "        # Instantiate NeuralController\n",
    "        try:\n",
    "            # NeuralController expects (model, tokenizer). For VLM we pass the model and processor.tokenizer\n",
    "            tokenizer_like = getattr(processor, 'tokenizer', None)\n",
    "            controller = NeuralController(model, tokenizer_like, control_method='logistic', batch_size=2)\n",
    "            controller.name = 'qwen3-vl'\n",
    "            print(\"NeuralController instantiated\")\n",
    "\n",
    "            # Try computing directions using the controller's compute_directions wrapper\n",
    "            try:\n",
    "                controller.compute_directions(X, y)\n",
    "                print(\"✅ NeuralController computed directions\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ NeuralController.compute_directions failed: {e}\")\n",
    "                print(\"Attempting to call toolkit directly (logistic mean-diff fallback)\")\n",
    "\n",
    "                # Fallback: compute mean-diff direction manually and store in controller.directions\n",
    "                cons_mean = cons_X.mean(axis=0, keepdims=True)\n",
    "                lib_mean = lib_X.mean(axis=0, keepdims=True)\n",
    "                direction = lib_mean - cons_mean\n",
    "                direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "                # store as single-layer dummy direction to be used by hook utilities\n",
    "                controller.directions = { -1: direction }\n",
    "                print(\"✅ Fallback mean-diff direction computed and stored in controller.directions\")\n",
    "\n",
    "            # Quick controlled generation example using controller.generate\n",
    "            try:\n",
    "                topics = [\"abortion rights\", \"climate change\"]\n",
    "                for topic in topics:\n",
    "                    prompt = f\"What is your view on {topic}?\"\n",
    "                    print(f\"\\n--- Controlled generate for: {topic} ---\")\n",
    "                    out = controller.generate(prompt, layers_to_control=[-1], control_coef=0.8, max_new_tokens=80)\n",
    "                    print(out)\n",
    "                print(\"\\n✅ Controller generation attempted\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Controlled generation failed: {e}\")\n",
    "                print(\"You may need to adapt `NeuralController` to the VLM model shape/API or use generation_utils hooks directly.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to create NeuralController: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steering_directions(cons_emb, lib_emb, method=\"logistic\"):\n",
    "    \"\"\"\n",
    "    Compute steering directions from embeddings.\n",
    "    \"\"\"\n",
    "    cons_emb = cons_emb.numpy() if isinstance(cons_emb, torch.Tensor) else cons_emb\n",
    "    lib_emb = lib_emb.numpy() if isinstance(lib_emb, torch.Tensor) else lib_emb\n",
    "    \n",
    "    if method == \"mean_diff\":\n",
    "        cons_mean = cons_emb.mean(axis=0, keepdims=True)\n",
    "        lib_mean = lib_emb.mean(axis=0, keepdims=True)\n",
    "        direction = lib_mean - cons_mean\n",
    "        print(f\"Method: Mean Difference\")\n",
    "    elif method == \"logistic\":\n",
    "        X = np.vstack([cons_emb, lib_emb])\n",
    "        y = np.array([0]*len(cons_emb) + [1]*len(lib_emb))\n",
    "        clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "        clf.fit(X, y)\n",
    "        direction = clf.coef_.reshape(1, -1)\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-10)\n",
    "        print(f\"Method: Logistic Regression\")\n",
    "    \n",
    "    return torch.tensor(direction, dtype=torch.float32)\n",
    "\n",
    "if 'cons_emb' in locals() and 'lib_emb' in locals() and len(cons_emb) > 0 and len(lib_emb) > 0:\n",
    "    print(\"\\n=== Computing Steering Directions ===\")\n",
    "    steering_direction = compute_steering_directions(cons_emb, lib_emb, method=\"logistic\")\n",
    "    print(f\"Steering direction shape: {steering_direction.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringHook:\n",
    "    \"\"\"Hook that modifies hidden states by adding a scaled direction vector\"\"\"\n",
    "    def __init__(self, direction, coefficient=1.0):\n",
    "        self.direction = direction.to(device)\n",
    "        self.coefficient = coefficient\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            return output + self.coefficient * self.direction\n",
    "        elif hasattr(output, 'last_hidden_state'):\n",
    "            output.last_hidden_state = output.last_hidden_state + self.coefficient * self.direction\n",
    "            return output\n",
    "        return output\n",
    "\n",
    "def apply_steering_to_model(model, steering_direction, coefficient=1.0, target_type=\"decoder\"):\n",
    "    \"\"\"Register steering hooks on specified model layers.\"\"\"\n",
    "    hook = SteeringHook(steering_direction, coefficient)\n",
    "    handles = []\n",
    "    \n",
    "    if target_type == \"decoder\":\n",
    "        if hasattr(model, 'language_model'):\n",
    "            decoder = model.language_model\n",
    "        elif hasattr(model, 'model'):\n",
    "            decoder = model.model\n",
    "        else:\n",
    "            decoder = model\n",
    "        \n",
    "        if hasattr(decoder, 'layers'):\n",
    "            layer_list = decoder.layers\n",
    "            num_layers = len(layer_list)\n",
    "            target_indices = [num_layers // 2, num_layers - 1]\n",
    "            \n",
    "            for idx in target_indices:\n",
    "                if idx < len(layer_list):\n",
    "                    h = layer_list[idx].register_forward_hook(hook)\n",
    "                    handles.append(h)\n",
    "                    print(f\"  Registered steering hook on decoder layer {idx}\")\n",
    "    \n",
    "    return handles, hook\n",
    "\n",
    "print(\"✓ Steering hook classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate examples with and without steering\n",
    "if 'steering_direction' in locals():\n",
    "    print(\"\\n=== Generation Examples ===\")\n",
    "    \n",
    "    test_topics = [\"abortion rights\", \"climate change\"]\n",
    "    \n",
    "    for steering_type, coef in [(\"BASELINE\", 0.0), (\"LIBERAL\", 1.0), (\"CONSERVATIVE\", -1.0)]:\n",
    "        print(f\"\\n--- {steering_type} (coefficient={coef}) ---\")\n",
    "        \n",
    "        if coef != 0.0:\n",
    "            handles, _ = apply_steering_to_model(model, steering_direction, coefficient=coef)\n",
    "        \n",
    "        for topic in test_topics:\n",
    "            prompt = f\"What is your view on {topic}?\"\n",
    "            inputs = processor(text=prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=80)\n",
    "            \n",
    "            text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"\\nTopic: {topic}\")\n",
    "            print(f\"Response: {text[:200]}...\")\n",
    "        \n",
    "        if coef != 0.0:\n",
    "            for h in handles: h.remove()\n",
    "    \n",
    "    print(\"\\n✅ Generation examples complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directions to disk\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if 'steering_direction' in locals():\n",
    "    print(\"\\n=== Saving Steering Directions ===\")\n",
    "    \n",
    "    directions_dir = Path(\"../directions\")\n",
    "    directions_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    direction_path = directions_dir / f\"conservative_liberal_direction_{timestamp}.pkl\"\n",
    "    with open(direction_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'direction': steering_direction.cpu().numpy(),\n",
    "            'direction_shape': steering_direction.shape,\n",
    "            'computation_method': 'logistic_regression',\n",
    "            'timestamp': timestamp,\n",
    "            'num_conservative_images': len(cons_emb),\n",
    "            'num_liberal_images': len(lib_emb),\n",
    "        }, f)\n",
    "    \n",
    "    metadata = {\n",
    "        'created_at': timestamp,\n",
    "        'method': 'logistic_regression',\n",
    "        'direction_file': str(direction_path),\n",
    "        'model': 'Qwen/Qwen3-VL-32B',\n",
    "        'political_axis': 'conservative -> liberal',\n",
    "    }\n",
    "    \n",
    "    metadata_path = directions_dir / f\"metadata_{timestamp}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Direction saved to: {direction_path}\")\n",
    "    print(f\"✅ Metadata saved to: {metadata_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi_nc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
